{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tools import categorical\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n\nimport xgboost\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-25T07:47:11.343447Z","iopub.execute_input":"2022-11-25T07:47:11.344361Z","iopub.status.idle":"2022-11-25T07:47:11.355054Z","shell.execute_reply.started":"2022-11-25T07:47:11.344321Z","shell.execute_reply":"2022-11-25T07:47:11.353755Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/mmf-data-science-2022/sample_submission.csv\n/kaggle/input/mmf-data-science-2022/train.csv\n/kaggle/input/mmf-data-science-2022/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/mmf-data-science-2022/train.csv', index_col=0)\ndf_train = pd.get_dummies(df_train)\n\nX = df_train.drop('price', axis=1)\ny = df_train['price']\n\ndf_test = pd.read_csv('../input/mmf-data-science-2022/test.csv', index_col='id')\ndf_test = pd.get_dummies(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T07:47:13.518539Z","iopub.execute_input":"2022-11-25T07:47:13.519029Z","iopub.status.idle":"2022-11-25T07:47:13.642668Z","shell.execute_reply.started":"2022-11-25T07:47:13.518983Z","shell.execute_reply":"2022-11-25T07:47:13.641322Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T07:47:17.139030Z","iopub.execute_input":"2022-11-25T07:47:17.139411Z","iopub.status.idle":"2022-11-25T07:47:17.158148Z","shell.execute_reply.started":"2022-11-25T07:47:17.139381Z","shell.execute_reply":"2022-11-25T07:47:17.156994Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lin_model = sm.OLS(y_train, X_train).fit()\nprint(lin_model.summary())\n\nvif_summary = pd.DataFrame()\nvif_summary[\"predictor\"] = X_train.columns\nvif_summary[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(len(X_train.columns))]\n  \nprint(vif_summary)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T07:49:26.216005Z","iopub.execute_input":"2022-11-25T07:49:26.216487Z","iopub.status.idle":"2022-11-25T07:49:29.514613Z","shell.execute_reply.started":"2022-11-25T07:49:26.216452Z","shell.execute_reply":"2022-11-25T07:49:29.513125Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.920\nModel:                            OLS   Adj. R-squared:                  0.920\nMethod:                 Least Squares   F-statistic:                 1.614e+04\nDate:                Fri, 25 Nov 2022   Prob (F-statistic):               0.00\nTime:                        07:49:26   Log-Likelihood:            -2.7342e+05\nNo. Observations:               32365   AIC:                         5.469e+05\nDf Residuals:                   32341   BIC:                         5.471e+05\nDf Model:                          23                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\ncarat          1.136e+04     63.168    179.817      0.000    1.12e+04    1.15e+04\ndepth           -61.3403      6.966     -8.806      0.000     -74.993     -47.687\ntable           -24.5539      3.777     -6.500      0.000     -31.958     -17.150\nx              -987.6829     52.478    -18.821      0.000   -1090.542    -884.824\ny                -3.6337     20.095     -0.181      0.857     -43.021      35.753\nz              -131.2061     76.744     -1.710      0.087    -281.628      19.216\ncut_Fair       1806.1302    251.223      7.189      0.000    1313.723    2298.537\ncut_Good       2427.6021    242.932      9.993      0.000    1951.447    2903.758\ncut_Ideal      2676.4077    233.533     11.461      0.000    2218.675    3134.140\ncut_Premium    2594.9679    238.568     10.877      0.000    2127.367    3062.569\ncut_Very Good  2579.8505    238.194     10.831      0.000    2112.982    3046.719\ncolor_D        2549.0630    172.101     14.811      0.000    2211.740    2886.387\ncolor_E        2355.4355    171.865     13.705      0.000    2018.574    2692.297\ncolor_F        2282.7917    171.975     13.274      0.000    1945.715    2619.869\ncolor_G        2058.7069    172.149     11.959      0.000    1721.288    2396.126\ncolor_H        1560.2671    172.462      9.047      0.000    1222.236    1898.298\ncolor_I        1101.2496    172.579      6.381      0.000     762.988    1439.512\ncolor_J         177.4445    173.444      1.023      0.306    -162.513     517.402\nclarity_I1    -2312.9355    159.662    -14.486      0.000   -2625.879   -1999.992\nclarity_IF     3069.1979    151.581     20.248      0.000    2772.093    3366.302\nclarity_SI1    1353.9069    151.693      8.925      0.000    1056.582    1651.232\nclarity_SI2     404.5484    151.734      2.666      0.008     107.143     701.953\nclarity_VS1    2276.0022    151.009     15.072      0.000    1980.019    2571.985\nclarity_VS2    1966.6253    151.085     13.017      0.000    1670.493    2262.758\nclarity_VVS1   2681.2662    150.823     17.778      0.000    2385.648    2976.884\nclarity_VVS2   2646.3470    150.820     17.546      0.000    2350.734    2941.959\n==============================================================================\nOmnibus:                     8915.447   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           220973.313\nSkew:                           0.766   Prob(JB):                         0.00\nKurtosis:                      15.709   Cond. No.                     1.16e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 1.73e-24. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/stats/outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n  vif = 1. / (1. - r_squared_i)\n","output_type":"stream"},{"name":"stdout","text":"        predictor        VIF\n0           carat  22.645690\n1           depth   2.500656\n2           table   1.781998\n3               x  87.796379\n4               y  13.786220\n5               z  72.195249\n6        cut_Fair        inf\n7        cut_Good        inf\n8       cut_Ideal        inf\n9     cut_Premium        inf\n10  cut_Very Good        inf\n11        color_D        inf\n12        color_E        inf\n13        color_F        inf\n14        color_G        inf\n15        color_H        inf\n16        color_I        inf\n17        color_J        inf\n18     clarity_I1        inf\n19     clarity_IF        inf\n20    clarity_SI1        inf\n21    clarity_SI2        inf\n22    clarity_VS1        inf\n23    clarity_VS2        inf\n24   clarity_VVS1        inf\n25   clarity_VVS2        inf\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = lin_model.predict(X_test)\nprint(mean_squared_error(y_train, lin_model.predict(X_train)))\nprint(mean_squared_error(y_pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-11-25T08:02:02.471371Z","iopub.execute_input":"2022-11-25T08:02:02.471861Z","iopub.status.idle":"2022-11-25T08:02:02.494719Z","shell.execute_reply.started":"2022-11-25T08:02:02.471824Z","shell.execute_reply":"2022-11-25T08:02:02.493007Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"1274991.4538794998\n1348035.1360985942\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(X, y, n_estimators=330, max_depth=9, eta=0.05, subsample=0.7, colsample_bytree=0.8):\n    model = XGBRegressor()\n    model = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample_bytree)\n    model.fit(X, y)\n    return model\n\nmodel = train_model(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(y_pred)\n\nprint(mean_squared_error(y_train, model.predict(X_train)))\nprint(mean_squared_error(y_pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-11-21T00:05:31.114417Z","iopub.execute_input":"2022-11-21T00:05:31.114983Z","iopub.status.idle":"2022-11-21T00:05:49.180164Z","shell.execute_reply.started":"2022-11-21T00:05:31.114935Z","shell.execute_reply":"2022-11-21T00:05:49.178326Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[1785.1399  543.3673  885.4657 ...  428.0165  751.5482 9522.319 ]\n69672.18630930065\n288771.3206776527\n","output_type":"stream"}]},{"cell_type":"code","source":"# K-Fold cross validation\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\nprint(scores)\n\n# within a loop\nest_range = np.arange(100, 1000, 100)\nprint(est_range)\n\nscoress = []\nfor est in est_range:\n    model = train_model(X_train, y_train, n_estimator=est)\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    scoress.append(scores)\n\nprint(scoress)","metadata":{"execution":{"iopub.status.busy":"2022-11-21T00:05:49.184879Z","iopub.execute_input":"2022-11-21T00:05:49.187445Z","iopub.status.idle":"2022-11-21T00:12:26.065523Z","shell.execute_reply.started":"2022-11-21T00:05:49.187390Z","shell.execute_reply":"2022-11-21T00:12:26.063871Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[-259.34341034 -255.42492117 -279.25893889 -272.93072227 -276.66135452\n -276.51728552 -261.00141639 -266.05908697 -265.44402116 -276.42644669\n -250.70892408 -271.88447065 -277.00800338 -265.8290423  -264.66106111\n -273.23961151 -278.89760647 -272.04635194 -270.00804904 -268.80424843\n -266.04429463 -259.36418677 -278.63502405 -265.42998727 -266.8768582\n -263.15496397 -271.81159373 -274.34522189 -266.04877967 -272.86599622]\n[100 200 300 400 500 600 700 800 900]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/150676971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscoress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mest_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'n_estimator'"],"ename":"TypeError","evalue":"train_model() got an unexpected keyword argument 'n_estimator'","output_type":"error"}]},{"cell_type":"code","source":"# real test\ncomp_model = train_model(X, y)\n\ny_pred = comp_model.predict(df_test)\nprediction = pd.DataFrame(y_pred, columns=['predictions']).to_csv('/kaggle/working/prediction.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-21T00:12:26.067659Z","iopub.status.idle":"2022-11-21T00:12:26.068169Z","shell.execute_reply.started":"2022-11-21T00:12:26.067950Z","shell.execute_reply":"2022-11-21T00:12:26.067970Z"},"trusted":true},"execution_count":null,"outputs":[]}]}